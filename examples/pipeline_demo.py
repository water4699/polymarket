#!/usr/bin/env python3
"""
PredictLab ç®¡é“è°ƒåº¦æ¼”ç¤º
å±•ç¤ºå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹å’Œé”™è¯¯å¤„ç†æœºåˆ¶
"""
import asyncio
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from modules.scheduler.task_scheduler import DataPipelineScheduler, Task, TaskStatus
from utils.logger import get_logger

logger = get_logger(__name__)


async def demo_successful_pipeline():
    """æ¼”ç¤ºæˆåŠŸçš„ç®¡é“æ‰§è¡Œ"""
    print("\n" + "="*60)
    print("ğŸš€ æ¼”ç¤º1: æˆåŠŸçš„æ•°æ®ç®¡é“æ‰§è¡Œ")
    print("="*60)

    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = DataPipelineScheduler()

    # åˆ›å»ºæ¨¡æ‹Ÿç®¡é“é…ç½®
    pipeline_config = {
        'symbols': ['BTC_PRICE'],  # åªå¤„ç†ä¸€ä¸ªèµ„äº§ä»¥åŠ å¿«æ¼”ç¤º
        'source_types': ['predict'],
        'intervals': ['1h'],
        'days_back': 1,  # åªå¤„ç†1å¤©æ•°æ®
    }

    # åˆ›å»ºç®¡é“
    scheduler.create_data_pipeline(pipeline_config)

    print(f"åˆ›å»ºäº† {len(scheduler.tasks)} ä¸ªä»»åŠ¡")
    print("ä»»åŠ¡åˆ—è¡¨:")
    for task_id, task in scheduler.tasks.items():
        deps = ", ".join(task.dependencies) if task.dependencies else "æ— "
        print(f"  â€¢ {task.name} (ä¾èµ–: {deps})")

    # æ‰§è¡Œç®¡é“
    print("\nå¼€å§‹æ‰§è¡Œç®¡é“...")
    start_time = datetime.now()

    results = await scheduler.execute_pipeline(max_concurrent=2)

    execution_time = (datetime.now() - start_time).total_seconds()

    # æ˜¾ç¤ºç»“æœ
    print(".2f")
    print("æ‰§è¡Œç»“æœ:")

    for task_id, result in results.items():
        status_emoji = {
            TaskStatus.SUCCESS: "âœ…",
            TaskStatus.FAILED: "âŒ",
            TaskStatus.SKIPPED: "â­ï¸",
            TaskStatus.RETRYING: "ğŸ”„"
        }.get(result.status, "â“")

        print("6.2f")
        if result.error:
            print(f"      é”™è¯¯: {result.error}")


async def demo_error_handling():
    """æ¼”ç¤ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
    print("\n" + "="*60)
    print("ğŸ›¡ï¸ æ¼”ç¤º2: é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
    print("="*60)

    scheduler = DataPipelineScheduler()

    # æ·»åŠ ä¸€ä¸ªå¿…å®šå¤±è´¥çš„ä»»åŠ¡
    async def failing_task():
        raise Exception("è¿™æ˜¯ä¸€ä¸ªæ•…æ„åˆ¶é€ çš„é”™è¯¯ï¼Œç”¨äºæ¼”ç¤ºé‡è¯•æœºåˆ¶")

    scheduler.add_task(Task(
        task_id="failing_task",
        name="å¿…å®šå¤±è´¥çš„ä»»åŠ¡",
        func=failing_task,
        max_retries=3,
        retry_delay=0.5,  # å¿«é€Ÿé‡è¯•ä»¥åŠ å¿«æ¼”ç¤º
        critical=False
    ))

    # æ·»åŠ ä¾èµ–äºå¤±è´¥ä»»åŠ¡çš„ä»»åŠ¡
    async def dependent_task():
        return "ä¾èµ–ä»»åŠ¡æˆåŠŸæ‰§è¡Œ"

    scheduler.add_task(Task(
        task_id="dependent_task",
        name="ä¾èµ–ä»»åŠ¡",
        func=dependent_task,
        dependencies=["failing_task"]
    ))

    # æ‰§è¡Œ
    results = await scheduler.execute_pipeline(max_concurrent=1)

    # åˆ†æç»“æœ
    failing_result = results.get("failing_task")
    dependent_result = results.get("dependent_task")

    print("å¤±è´¥ä»»åŠ¡ç»“æœ:")
    print(f"  çŠ¶æ€: {failing_result.status.value}")
    print(f"  é‡è¯•æ¬¡æ•°: {failing_result.retry_count}")
    print(f"  æ‰§è¡Œæ—¶é—´: {failing_result.duration:.2f}ç§’")
    print(f"  é”™è¯¯ä¿¡æ¯: {failing_result.error}")

    print("\nä¾èµ–ä»»åŠ¡ç»“æœ:")
    print(f"  çŠ¶æ€: {dependent_result.status.value}")
    print("  (ä¾èµ–ä»»åŠ¡è¢«è·³è¿‡ï¼Œå› ä¸ºä¸Šæ¸¸ä»»åŠ¡å¤±è´¥)")

    print("\nğŸ’¡ é”™è¯¯å¤„ç†ç‰¹æ€§:")
    print("  â€¢ è‡ªåŠ¨é‡è¯•æœºåˆ¶ (æœ€å¤š3æ¬¡)")
    print("  â€¢ ä¾èµ–æ£€æŸ¥é˜²æ­¢çº§è”é”™è¯¯")
    print("  â€¢ è¯¦ç»†çš„é”™è¯¯æ—¥å¿—è®°å½•")


async def demo_concurrent_execution():
    """æ¼”ç¤ºå¹¶å‘æ‰§è¡Œ"""
    print("\n" + "="*60)
    print("âš¡ æ¼”ç¤º3: å¹¶å‘æ‰§è¡Œä¼˜åŒ–")
    print("="*60)

    scheduler = DataPipelineScheduler()

    # æ·»åŠ å¤šä¸ªç‹¬ç«‹ä»»åŠ¡æ¥æ¼”ç¤ºå¹¶å‘
    async def quick_task(task_id: str, duration: float):
        await asyncio.sleep(duration)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        return f"ä»»åŠ¡ {task_id} å®Œæˆï¼Œè€—æ—¶ {duration}ç§’"

    # åˆ›å»º10ä¸ªå¿«é€Ÿä»»åŠ¡
    for i in range(10):
        scheduler.add_task(Task(
            task_id=f"quick_task_{i}",
            name=f"å¿«é€Ÿä»»åŠ¡ {i}",
            func=quick_task,
            args=[f"task_{i}", 0.1 * (i % 3 + 1)]  # ä¸åŒçš„å¤„ç†æ—¶é—´
        ))

    # ä¸²è¡Œæ‰§è¡Œ (å¹¶å‘æ•°=1)
    print("ä¸²è¡Œæ‰§è¡Œ (max_concurrent=1):")
    start_time = datetime.now()
    results_serial = await scheduler.execute_pipeline(max_concurrent=1)
    serial_time = (datetime.now() - start_time).total_seconds()

    # é‡æ–°åˆ›å»ºä»»åŠ¡
    scheduler.tasks.clear()
    scheduler.task_results.clear()
    scheduler.task_status.clear()
    for i in range(10):
        scheduler.add_task(Task(
            task_id=f"quick_task_{i}",
            name=f"å¿«é€Ÿä»»åŠ¡ {i}",
            func=quick_task,
            args=[f"task_{i}", 0.1 * (i % 3 + 1)]
        ))

    # å¹¶å‘æ‰§è¡Œ (å¹¶å‘æ•°=3)
    print("å¹¶å‘æ‰§è¡Œ (max_concurrent=3):")
    start_time = datetime.now()
    results_concurrent = await scheduler.execute_pipeline(max_concurrent=3)
    concurrent_time = (datetime.now() - start_time).total_seconds()

    # æ¯”è¾ƒç»“æœ
    speedup = serial_time / concurrent_time if concurrent_time > 0 else 1

    print(".2f")
    print(".2f")
    print(".1f")

    print("
ğŸ’¡ å¹¶å‘æ‰§è¡Œä¼˜åŠ¿:"    print("  â€¢ ç‹¬ç«‹ä»»åŠ¡å¯å¹¶è¡Œå¤„ç†")
    print("  â€¢ æ˜¾è‘—æå‡æ•´ä½“ throughput")
    print("  â€¢ è‡ªåŠ¨èµ„æºæ± ç®¡ç†")


async def demo_pipeline_monitoring():
    """æ¼”ç¤ºç®¡é“ç›‘æ§"""
    print("\n" + "="*60)
    print("ğŸ“Š æ¼”ç¤º4: ç®¡é“ç›‘æ§å’ŒçŠ¶æ€è·Ÿè¸ª")
    print("="*60)

    scheduler = DataPipelineScheduler()

    # åˆ›å»ºä¸€ä¸ªå°å‹ç®¡é“
    pipeline_config = {
        'symbols': ['BTC_PRICE', 'ETH_PRICE'],
        'source_types': ['predict'],
        'intervals': ['1h'],
        'days_back': 1,
    }

    scheduler.create_data_pipeline(pipeline_config)

    # æ¨¡æ‹Ÿæ‰§è¡Œè¿‡ç¨‹ï¼Œå®šæœŸæ£€æŸ¥çŠ¶æ€
    print("å¯åŠ¨ç®¡é“æ‰§è¡Œ...")
    execution_task = asyncio.create_task(scheduler.execute_pipeline(max_concurrent=2))

    # ç›‘æ§çŠ¶æ€
    while not execution_task.done():
        status = scheduler.get_pipeline_status()
        progress = status['progress']
        completed = status['completed_tasks']
        total = status['total_tasks']

        print("5")

        await asyncio.sleep(0.5)  # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡

    # ç­‰å¾…æ‰§è¡Œå®Œæˆ
    results = await execution_task

    # æœ€ç»ˆçŠ¶æ€æŠ¥å‘Š
    final_status = scheduler.get_pipeline_status()

    print("
æœ€ç»ˆæ‰§è¡ŒæŠ¥å‘Š:"    print(f"  æ€»ä»»åŠ¡æ•°: {final_status['total_tasks']}")
    print(f"  å®Œæˆä»»åŠ¡: {final_status['completed_tasks']}")
    print(".1%")

    print("
ä»»åŠ¡çŠ¶æ€åˆ†å¸ƒ:"    for status_name, count in final_status['status_breakdown'].items():
        if count > 0:
            print(f"  {status_name}: {count}")

    print("
ğŸ“ˆ ç›‘æ§ç‰¹æ€§:"    print("  â€¢ å®æ—¶è¿›åº¦è·Ÿè¸ª")
    print("  â€¢ ä»»åŠ¡çŠ¶æ€ç»Ÿè®¡")
    print("  â€¢ æ€§èƒ½æŒ‡æ ‡ç›‘æ§")


async def demo_custom_pipeline():
    """æ¼”ç¤ºè‡ªå®šä¹‰ç®¡é“é…ç½®"""
    print("\n" + "="*60)
    print("ğŸ”§ æ¼”ç¤º5: è‡ªå®šä¹‰ç®¡é“é…ç½®")
    print("="*60)

    scheduler = DataPipelineScheduler()

    # è‡ªå®šä¹‰ä»»åŠ¡é…ç½®
    custom_config = {
        'symbols': ['BTC_PRICE'],
        'source_types': ['predict', 'polymarket'],  # å¤šæ•°æ®æº
        'intervals': ['1h', '4h', '1d'],  # å¤šæ—¶é—´å‘¨æœŸ
        'days_back': 3,
    }

    # åˆ›å»ºè‡ªå®šä¹‰ç®¡é“
    scheduler.create_data_pipeline(custom_config)

    print(f"è‡ªå®šä¹‰ç®¡é“åŒ…å« {len(scheduler.tasks)} ä¸ªä»»åŠ¡:")

    # æŒ‰é˜¶æ®µåˆ†ç»„æ˜¾ç¤º
    stages = {}
    for task_id, task in scheduler.tasks.items():
        stage = task_id.split('_')[0]
        if stage not in stages:
            stages[stage] = []
        stages[stage].append(task.name)

    for stage, tasks in stages.items():
        print(f"  {stage.upper()}: {len(tasks)} ä¸ªä»»åŠ¡")
        for task in tasks[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"    â€¢ {task}")
        if len(tasks) > 3:
            print(f"    ... è¿˜æœ‰ {len(tasks) - 3} ä¸ªä»»åŠ¡")

    # æ˜¾ç¤ºä¾èµ–å…³ç³»
    print("
ä¾èµ–å…³ç³»ç¤ºä¾‹:"    for task_id, task in list(scheduler.tasks.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
        deps = ", ".join(task.dependencies) if task.dependencies else "æ— ä¾èµ–"
        print(f"  {task.name} â†’ ä¾èµ–: {deps}")

    print("
ğŸ›ï¸ è‡ªå®šä¹‰é…ç½®ç‰¹æ€§:"    print("  â€¢ å¤šæ•°æ®æºå¹¶è¡Œå¤„ç†")
    print("  â€¢ å¤šæ—¶é—´å‘¨æœŸKçº¿ç”Ÿæˆ")
    print("  â€¢ çµæ´»çš„ä»»åŠ¡ä¾èµ–é…ç½®")
    print("  â€¢ å¯æ‰©å±•çš„ç®¡é“æ¶æ„")


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ¯ PredictLab ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿæ¼”ç¤º")
    print("å±•ç¤ºæ•°æ®ç®¡é“çš„æ‰§è¡Œé¡ºåºã€ä¾èµ–ç®¡ç†å’Œé”™è¯¯å¤„ç†")

    try:
        # æ¼”ç¤º1: æˆåŠŸç®¡é“
        await demo_successful_pipeline()

        # æ¼”ç¤º2: é”™è¯¯å¤„ç†
        await demo_error_handling()

        # æ¼”ç¤º3: å¹¶å‘æ‰§è¡Œ
        await demo_concurrent_execution()

        # æ¼”ç¤º4: ç®¡é“ç›‘æ§
        await demo_pipeline_monitoring()

        # æ¼”ç¤º5: è‡ªå®šä¹‰ç®¡é“
        await demo_custom_pipeline()

        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("="*60)
        print("æ ¸å¿ƒç‰¹æ€§æ€»ç»“:")
        print("âœ… ä»»åŠ¡ä¾èµ–ç®¡ç†å’Œæ‹“æ‰‘æ’åº")
        print("âœ… å¼‚æ­¥å¹¶å‘æ‰§è¡Œå’Œèµ„æºæ§åˆ¶")
        print("âœ… è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯å¤„ç†æœºåˆ¶")
        print("âœ… å®æ—¶ç›‘æ§å’ŒçŠ¶æ€è·Ÿè¸ª")
        print("âœ… çµæ´»çš„ç®¡é“é…ç½®ç³»ç»Ÿ")
        print("âœ… æ•°æ®ä¸€è‡´æ€§ä¿éšœ")

    except Exception as e:
        logger.error(f"æ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"ğŸ’¥ æ¼”ç¤ºå¤±è´¥: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
